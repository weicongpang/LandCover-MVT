# -*- coding: utf-8 -*-
import json
import os
import random
from typing import Any, Dict, List

# Input paths
INPUT_JSON = "/root/openset/dataset_processed/output_json/renamed_output_images_annotation.json"
IMAGE_CATEGORY_MAPPING_JSON = "/root/openset/dataset/renamed_final/image_category_mapping.json"

# Output paths
OUTPUT_DIR = "/root/openset/llama_factory/LLaMA-Factory/data"
DATASET_FILE = "rs_open_tag_infer_new.jsonl"

INCLUDE_RLE = True
MIN_SCORE_HINT = 0.30

def load_json(path: str) -> List[Dict[str, Any]]:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def build_instruction() -> str:
    return (
        "You are a remote-sensing imagery expert. <image>\n\n"
        "You are provided with an image accompanied by detailed segmentation results generated by the SAM model. "
        "Each segment includes:\n"
        "- 'bbox': bounding box coordinates [x,y,w,h], approximately locating the segmented object;\n"
        "- 'area': the area (in pixels) of the segmented object;\n"
        "- 'counts': pixel-level RLE (Run-Length Encoding) mask precisely defining object boundaries;\n"
        "- 'score': confidence score assigned by SAM (optional).\n\n"

        "STRICTLY follow these INTERNAL THINKING STEPS (do NOT output your thinking process, these steps are for internal reasoning ONLY):\n\n"

        "Step 1: INTERNALLY analyze the image comprehensively by observing its overall textures, patterns, and visual structure. "
        "Carefully consider the provided segmentation results (bbox, area, pixel-level masks) to identify pixel-level, object-level and region-level features.\n\n"

        "Step 2: Based on your analysis, INTERNALLY select the MOST APPROPRIATE first-level land-use category from the following standardized list:\n"
        "[Cultivated Land, Garden Land, Forest Land, Grassland, Commercial Service Land, Industrial, Mining, and Storage Land, Residential Land, Public Administration and Public Service Land, Special Land, Transportation Land, Water Bodies and Hydraulic Facility Land, Other Land].\n\n"

        "Step 3: INTERNALLY determine the DETAILED subtype within the selected first-level land-use category, precisely guided by the segmentation information provided by SAM.\n\n"

        "Step 4: INTERNALLY form a clear and precise description of this image, explicitly leveraging the SAM segmentation results (bbox locations, mask shapes, areas, and pixel-level information) to consolidate your understanding and classification decision.\n\n"

        "IMPORTANT OUTPUT REQUIREMENT:\n"
        "You MUST ONLY OUTPUT the final classification category label corresponding to the detailed land-use subtype (e.g., category0011). "
        "DO NOT OUTPUT any intermediate reasoning, first-level categories, subtype names, or image descriptions. "
        "ONLY OUTPUT a single category ID."
    )

def build_input_payload(item: Dict[str, Any]) -> str:
    instances = []
    for ann in item.get("annotations", []):
        one = {
            "id": int(ann.get("id")),
            "bbox_xywh": ann.get("bbox"),
            "area": ann.get("area"),
            "iscrowd": ann.get("iscrowd", 0),
        }
        if "score" in ann:
            one["score"] = ann["score"]

        if INCLUDE_RLE and "segmentation" in ann:
            seg = ann.get("segmentation", {})
            one["segmentation"] = {
                "size": seg.get("size"),
                "counts": seg.get("counts"),
            }
        instances.append(one)

    payload = {
        "image_meta": {
            "image_id": int(item.get("id")),
            "width": item.get("width"),
            "height": item.get("height"),
            "path": item.get("file_path"),
            "instance_count": len(instances)
        },
        "instances": instances,
        "rules": {
            "labels_per_instance": 1,
            "enforce_one_label": True,
            "preserve_order": True,
            "language": "en",
            "min_score_hint": MIN_SCORE_HINT
        }
    }
    return json.dumps(payload, ensure_ascii=False)

def build_image_category_map(mapping_items: List[Dict[str, Any]]) -> Dict[str, str]:
    """
    Creates a mapping: new_path -> category.
    """
    return {item["new_path"]: item["category"] for item in mapping_items}

def convert_to_alpaca(
    items: List[Dict[str, Any]], image_category_map: Dict[str, str]
) -> List[Dict[str, Any]]:
    inst = build_instruction()
    outputs: List[Dict[str, Any]] = []

    for it in items:
        file_path = it["file_path"]
        category_label = image_category_map.get(file_path, "unknown")
        if category_label == "unknown":
            raise ValueError(f"Category not found for image path: {file_path}")

        sample = {
            "instruction": inst,
            "input": build_input_payload(it),
            "output": category_label,  # from mapping JSON
            "images": [file_path]
        }
        outputs.append(sample)

    return outputs

def save_jsonl(rows: List[Dict[str, Any]], path: str) -> None:
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        for r in rows:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

def main():
    items = load_json(INPUT_JSON)
    mapping_items = load_json(IMAGE_CATEGORY_MAPPING_JSON)

    image_category_map = build_image_category_map(mapping_items)

    print(f"[LOAD] Read {len(items)} image records from annotation JSON.")
    print(f"[LOAD] Read {len(mapping_items)} category mappings.")

    samples = convert_to_alpaca(items, image_category_map)

    random.shuffle(samples)
    print(f"[SHUFFLE] Shuffled {len(samples)} samples.")

    os.makedirs(OUTPUT_DIR, exist_ok=True)
    out_path = os.path.join(OUTPUT_DIR, DATASET_FILE)
    save_jsonl(samples, out_path)

    print(f"[SAVE] Dataset successfully written to: {out_path}")

if __name__ == "__main__":
    main()
